# TP5 â€“ Deep Reinforcement Learning

## Exploration de Gymnasium

> *python TP5/random_agent.py*

![Screenshot](./gifs/random_agent.gif)

![Screenshot](./screenshots/P1.jpg)

- L'agent alÃ©atoire obtient -231.81 points, soit 431 points en dessous du seuil de rÃ©solution (+200).

## EntraÃ®nement et Ã‰valuation de l'Agent PPO (Stable Baselines3)

> *python TP5/train_and_eval_ppo.py*

```
--- RAPPORT DE VOL PPO ---
Issue du vol : TEMPS Ã‰COULÃ‰ OU SORTIE DE ZONE âš ï¸
RÃ©compense totale cumulÃ©e : 58.34 points
Allumages moteur principal : 139
Allumages moteurs latÃ©raux : 370
DurÃ©e du vol : 1000 frames
VidÃ©o sauvegardÃ©e sous 'TP5/trained_ppo_agent.gif'
```

![Screenshot](./gifs/trained_ppo_agent.gif)


- La rÃ©compense moyenne est passÃ©e de -139 (dÃ©but, ~43k steps) Ã  +188 (fin, ~497k steps), soit une progression de +327 points. Il ne crashe plus (contrairement Ã  l'agent alÃ©atoire), stabilise son vol et tente de se positionner au-dessus de la zone d'atterrissage. Cependant, le score final d'Ã©valuation (58.34 points) reste en dessous du seuil de +200.


| MÃ©trique | Agent alÃ©atoire | Agent PPO |
|----------|----------------|-----------|
| Issue | CRASH | Timeout |
| Score | -231.81 | +58.34 |
| Moteur principal | 30 | 139 |
| Moteurs latÃ©raux | 60 | 370 |
| DurÃ©e | 115 frames | 1000 frames |

L'agent PPO survit 8Ã— plus longtemps et score 290 points de plus. Il utilise davantage 
ses moteurs pour stabiliser sa trajectoire, ce qui explique la consommation de carburant plus Ã©levÃ©e. Le seuil +200 n'est pas atteint sur cet Ã©pisode, mais ep_rew_mean ~ 184 en fin d'entraÃ®nement indique que l'agent y parvient en moyenne sur l'ensemble des Ã©pisodes.

## L'Art du Reward Engineering

> *python TP5/reward_hacker.py*

```
--- RAPPORT DE VOL PPO HACKED ---
Issue du vol : CRASH DÃ‰TECTÃ‰ ğŸ’¥
RÃ©compense totale cumulÃ©e : -96.60 points
Allumages moteur principal : 0
Allumages moteurs latÃ©raux : 42
DurÃ©e du vol : 63 frames
VidÃ©o sauvegardÃ©e sous 'TP5/hacked_agent.gif'
```

![Screenshot](./gifs/hacked_agent.gif)

- L'agent n'a pas du tout allumÃ© le moteur principal . Il utilise uniquement les moteurs latÃ©raux (42 fois) pour tenter de se stabiliser latÃ©ralement, mais sans poussÃ©e verticale, la chute est inÃ©vitable.

- La fonction de rÃ©compense modifiÃ©e inflige -50 points Ã  chaque activation du moteur principal (action=2), contre seulement -100 points pour un crash terminal. Du point de vue de l'agent, allumer le moteur principal mÃªme 2 fois coÃ»te dÃ©jÃ  -100, soit autant qu'un crash. L'optimum mathÃ©matique de la rÃ©compense modifiÃ©e est donc de ne jamais allumer le moteur principal.

- C'est un exemple classique de reward hacking: l'agent a trouvÃ© l'optimal de cette fonction de rÃ©compense, qui n'est pas l'optimal de 
notre intention.

## Robustesse et Changement de Physique

> *python TP5/ood_agent.py*

![Screenshot](./gifs/ood_agent.gif)


```
--- Ã‰VALUATION OOD : GRAVITÃ‰ FAIBLE ---

--- RAPPORT DE VOL PPO (GRAVITÃ‰ MODIFIÃ‰E) ---
Issue du vol : ATTERRISSAGE RÃ‰USSI ğŸ†
RÃ©compense totale cumulÃ©e : 252.75 points
Allumages moteur principal : 45
Allumages moteurs latÃ©raux : 225
DurÃ©e du vol : 379 frames
VidÃ©o sauvegardÃ©e sous 'TP5/ood_agent.gif'
```

- L'agent rÃ©ussit l'atterrissage avec 252.75 points (au-dessus du seuil de rÃ©solution). Le vol dure 379 frames (vs 1000 en gravitÃ© 
normale) et la consommation moteur est rÃ©duite (45 vs 139 allumages).

- Avec une gravitÃ© faible, le vaisseau descend lentement vers la zone d'atterrissage. La politique apprise en gravitÃ© -10.0 gÃ©nÃ¨re des poussÃ©es pour freiner une chute rapide. La gravitÃ© rÃ©duite simplifie le problÃ¨me plutÃ´t que de le complexifier. Par contre, une gravitÃ© plus forte ou un vent latÃ©ral ajoutÃ© produirait potentiellement l'effet inverse.

## Bilan IngÃ©nieur 

- StratÃ©gie 1: Varier la gravitÃ© pendant l'entraÃ®nement

Au lieu d'entraÃ®ner toujours avec gravity=-10.0, on pourrait tirer une valeur 
alÃ©atoire Ã  chaque Ã©pisode (par exemple entre -12.0 et -1.0). L'agent verrait des tas de physiques diffÃ©rentes pendant l'entraÃ®nement et apprendrait Ã  gÃ©rer toutes les situations.

- StratÃ©gie 2: Dire Ã  l'agent quelle est la gravitÃ©

Actuellement l'agent a 8 capteurs (position, vitesse, angle...) mais il ne sait pas dans quelle gravitÃ© il se trouve. On pourrait simplement ajouter la valeur de la gravitÃ© (et du vent s'il y en a) comme entrÃ©es supplÃ©mentaires du rÃ©seau. L'agent pourrait alors adapter directement sa politique: gravitÃ© faible => poussÃ©es lÃ©gÃ¨res, gravitÃ© forte => poussÃ©es plus importantes.
